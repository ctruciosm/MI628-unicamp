---
title: "Inferência Causal"
subtitle: "Propensity Score"
author: "Prof. Carlos Trucíos </br> ctrucios@unicamp.br"
Email: "ctrucios@unicamp.br"
institute: "Instituto de Matemática, Estatística e Computação Científica (IMECC), </br> Universidade Estadual de Campinas (UNICAMP)."
toc: true
toc-depth: 1
toc-title: "Conteúdo"
knitr:
    opts_chunk: 
      fig.align: 'center'
execute:
    message: false
    warning: false
format:
    revealjs:
        slide-number: true
        show-slide-number: print
        self-contained: false
        chalkboard: true
        width: 1600
        height: 900
        theme: [default, styles.scss]
        incremental: true
        code-fold: true
        logo: "imagens/imecc.png"
        footer: "Carlos Trucíos (IMECC/UNICAMP)  |  ME920/MI628 - Inferência Causal |  [ctruciosm.github.io](https://ctruciosm.github.io/)"
        highlight-style: "a11y"
        title-slide-attributes:
            data-background-image: "imagens/unicamp.png"
            data-background-size: 20%
            data-background-position: 99% 5%
            data-background-opacity: "1"
---



# Introdução
## Introdução

- Introduzido por Rosembaum e Rubin (1983), _propensity score_ desempenha um papel crucial na inferência causal quando trabalhamos com estudos observacionais.
- 


# Propensity score
## Propensity score

Assumindo uma amostragem IID, temos quatro variáveis aleatórias associadas a cada unidade $i$: 


- $X$, 
- $Z$, 
- $Y(1)$ e
- $Y(0)$.

. . . 

Ademais, $$P(X, Z, Y(1), Y(0)) = P(X) \times P(Y(1), Y(0) | X) \times P(Z | Y(1), Y(0), X)$$




## Propensity score

::: {.callout-tip}
### Definição (Propensity score):
Definimos $$e(X, Y(1), Y(0)) = P(Z = 1| X, Y(1), Y(0))$$ como o _propensity score_. Sob ignorabilidade forte, $$e(X, Y(1), Y(0)) = P(Z = 1| X, Y(1), Y(0)) = P(Z = 1 | X)$$ que é simplesmente denotado por $e(X)$ e é a **probabilidade condicional de receber o tratamento dado as covariáveis observadas**

:::

. . . 


<aside>
**Observação:** alguns autores definem _propensity score_ apenas como $e(1) = P(Z = 1 | X)$ seguindo a notação de Rosembaum e Rubin (1983). Essa notação é utilizada pois os autores focam em estudos observacionais. A definição $e(X, Y(1), Y(0)) = P(Z = 1| X, Y(1), Y(0))$ pode ser vista como uma generalização da definição de Rosembaum e Rubin (1983).
</aside>


# Propensity Score: Redução de Dimensão
## Propensity Score: Redução de Dimensão


::: {.callout-tip}
### Teorema
Se $Z \perp\!\!\!\perp  \{ Y(1), Y(0)\} | X$, então $Z \perp\!\!\!\perp  \{ Y(1), Y(0)\} | e(X)$

:::


. . . 


- O Teorema estabelece que, se ignorabilidade forte acontece (dado $X$), também acontece dado $e(X)$.
- Isto implica que condicionar em $e(X)$ remove qualquer efeito confundidor induzido por $X$.
- $X$ pode ser geral e multidimensional, já $e(X)$ é unidimensional e está entre 0 e 1.
- Então, propensity score reduz a dimensão das covariáveis originais mas ainda mantem a ignorabilidade.
- **Demostração** (no quadro)

## Propensity Score: Redução de Dimensão


O Teorema anterior motiva um método para estimar efeitos causais: **Propensity Score Stratification**.

. . . 

- Assuma que o _propensity score_ é conhecido e com apenas $K$ ($K << n$) possíveis valores $\{e_1, e_2, \cdots, e_K \}$.
- Neste caso, o Teorema anterior pode ser re-escrito como $$Z \perp\!\!\!\perp  \{ Y(1), Y(0)\} | e(X) = e_k \quad (k = 1, \cdots, K).$$
- Assim, temos $K$ CRES independentes em cada estrato do _propensity score_ e podemos analizar os dados observacionais da mesma forma como no SRE (mas agora estratificado em $e(X)$) `r emo::ji("smile")`


## Propensity Score: Redução de Dimensão


<center>
[Na prática, $e(X)$ não é conhecido e não é discreto `r emo::ji("sad")`.]{style="color:red"}
</center>

. . . 

### Como então utilizar propensity score para estratificar?

- Ajustamos um modelo para $P(Z = 1 | X)$, por exemplo, um modelo logístico.
- Os valores ajustados serão os _propensity score_ estimados, $\hat{e}(X)$.
-$\hat{e}(X)$ pode assumir tantos valores quanto o tamanho do conjunto de dados ($n$) `r emo::ji("sad")`.
- Contudo, podemos discretizar (utilizando os quantis ou algum outro método) $\hat{e}(X)$ e termos (aproximadamente) $$Z \perp\!\!\!\perp  \{ Y(1), Y(0)\} | \hat{e}'(X) = e_k \quad (k = 1, \cdots, K),$$ em que $\hat{e}'(X)$ são os quantis `r emo::ji("smile")`.


## Propensity Score: Redução de Dimensão

- Para obtermos $\hat{e}(X)$ precisamos ajustar um modelo, o que nos leva a pensar se o estimador final é dependente do modelo escolhido.
- Note que a estratificação utilizando _propensity score_ apenas precisa que a ordem esteja correta em lugar do que seus valores exatos.
- Isto torna o método um pouco robusto à escolha do modelo.

## Propensity Score: Redução de Dimensão


<center>
[Como escolher $K$?]{style="color:red"}
</center>

. . . 

- Rosenbaum and Rubin (1984) sugerem $K = 5$.
- Wang et al. (2020) sugerem escolher $K$ como o máximo número de estratos nos quais o estimador estratificado está bem definido.

. . . 

<center>
[Como calcular o desvio padrão?]{style="color:red"}
</center>

. . . 

- Alguns autores calculam o desvio padrão dentro de cada estrato obtido atraves de $\hat{e}(X)$, mas a incerteza associada a $\hat{e}(X)$ não é levada em consideração.
- Outros preferem fazer um Bootstrap para levar em consideração a incerteza associada a $\hat{e}(X)$.

## Propensity Score: Redução de Dimensão


::: {.callout-note}
### Exemplo

[_Dataset_ disponível aqui](https://dataverse.harvard.edu/file.xhtml?fileId=7440222&version=3.0). [Chang et al. (2016)](https://academic.oup.com/jrsssb/article/78/3/673/7040940?login=false) estudam se a participação no programa da merenda nas escolas (`School_meal`) leva a um aumento no índice de massa muscular (`BMI`) das crianças. 12 outras covariáveis também estão disponíveis no _dataset_.


```{r}
#| echo: true
dados <- read.csv("datasets/nhanes_bmi.csv")[, -1]
z <- dados$School_meal
y <- dados$BMI
x <- scale(dados[, -c(1, 2)])
```

:::


. . . 

Calculamos $\hat{e}(X)$

```{r}
#| echo: true
prop_score <- glm(z ~ x, family = binomial)$fitted.values 
```

## Propensity Score: Redução de Dimensão

Estratificamos e estimamos o efeito causal médio.

```{r}
#| echo: true
neyman_sre <- function(y, z, x) {
  x_levels <- unique(x)
  K <- length(x_levels)
  pi_k <- rep(0, K)
  tau_k <- rep(0, K)
  v_k <- rep(0, K)
  n <- length(z)
  for (k in 1:K) {
    x_k <- x_levels[k]
    z_k <- z[x == x_k]
    y_k <- y[x == x_k]
    pi_k[k] <- length(z_k)/n
    tau_k[k] <- mean(y_k[z_k == 1]) - mean(y_k[z_k == 0])
    v_k[k] <- var(y_k[z_k == 1]) / sum(z_k) + var(y_k[z_k == 0]) / sum(1 - z_k)
  }
  tau_s <- sum(pi_k * tau_k)
  v_s <- sum(pi_k^2 * v_k) 
  return(c(tau_s, sqrt(v_s)))
}

K <- c(5, 10, 20, 50, 80)
resultados <- sapply(K, FUN = function(k){
                          q_prop_score = quantile(prop_score, (1:(k - 1))/k)
                          ps_estrato = cut(prop_score, breaks = c(0, q_prop_score, 1), labels = 1:k)
                          neyman_sre(y, z, ps_estrato)
                          })
colnames(resultados) <- K
rownames(resultados) <- c("Estim", "SE")
resultados
```

## Propensity Score: Redução de Dimensão

Como seriam os resultados se não utilizamos _propensity score_?

- Naive: [MQO com correção para covariância](https://ctruciosm.github.io/MI628-unicamp/MI628_Aula03#/aplica%C3%A7%C3%A3o-1)
- [Fisher](https://ctruciosm.github.io/MI628-unicamp/MI628_Aula05#/ajuste-de-regress%C3%A3o-4)
- [Lin](https://ctruciosm.github.io/MI628-unicamp/MI628_Aula05#/ajuste-de-regress%C3%A3o-5) 


. . . 


```{r}
#| echo: true
library(car)
naive <- lm(y ~ z)
fisher <- lm(y ~ z + x)
lin <- lm(y ~ z*x)
resultados <- matrix(c(coef(naive)[2], sqrt(hccm(naive)[2, 2]),
                       coef(fisher)[2], sqrt(hccm(fisher)[2, 2]),
                       coef(lin)[2], sqrt(hccm(lin)[2, 2])), ncol = 3)
colnames(resultados) <- c("Naive", "Fisher", "Lin")
rownames(resultados) <- c("Estim", "SE")
resultados
```

. . . 


O estimador de Lin e o método de _propensity score stratification_ fornecem, qualitativamente, os mesmo resultados. Já o estimador Naive difere bastante dos outros métodos (**Why?**) enquanto que Fisher devolve um efeito positivo, porém, insignificante.


# Propensity Score: Ponderação
## Propensity Score: Ponderação


::: {.callout-tip}
### Teorema
Se $Z \perp\!\!\!\perp \{ Y(1), Y(0)\} | X$ e $0 < e(X) < 1$, então: $$\mathbb{E}[Y(1)] = \mathbb{E} \Big [ \dfrac{ZY}{e(X)}\Big], \quad \mathbb{E}[Y(0)] = \mathbb{E} \Big [ \dfrac{(1 - Z)Y}{1 - e(X)}\Big] \quad e$$ $$\tau = \mathbb{E}[Y(1) - Y(0)] = \mathbb{E} \Big [\dfrac{ZY}{e(X)} - \dfrac{(1-Z)Y}{1-e(X)} \Big]$$

:::

<aside>
A condição $0 < e(X) < 1$ é conhecida como a condição de sobreposição (*overlap*) ou positividade (*positivity*)
</aside>


**Demostração** (no quadro)




## Estimador IPW

O estimador IPW (Inverse Propensity Score Weighting), também conhecido como estimador HT (Horvitz-Thompson) é motivado pelo Teorema anterior e é dado por $$\hat{\tau}^{IPW} = \dfrac{1}{n} \displaystyle \sum_{i = 1}^n \dfrac{Z_i Y_i}{\hat{e}(X_i)} - \dfrac{1}{n} \displaystyle \sum_{i = 1}^n \dfrac{(1 - Z_i)Y_i}{1- \hat{e}(X_i)},$$ em que $\hat{e}(X_i)$ é o _propensity score_ estimado.




## Estimador IPW

- Suponha que em lugar de termos  resultado observado $Y_i$, temos $\tilde{Y}_i = Y_i + c$ ($\forall i$).
- Adicionar, para cada $i$, uma constante ao resultado observado não deveria mudar o efeito causal médio.
- Contudo, se adicionarmos, para cada $i$, uma constante $c$, temos que $\hat{\tau}^{IPW}$ torna-se $$\hat{\tau}^{IPW} + c \Big[\dfrac{1}{n} \displaystyle \sum_{i = 1}^n \dfrac{Z_i}{\hat{e}(X_i)} - \dfrac{1}{n} \displaystyle \sum_{i = 1}^n \dfrac{1 - Z_i}{1 - \hat{e}(X_i)} \Big]$$


. . . 

[O estimador $\hat{\tau}^{IPW}$ apresenta um sério problema: **não é invariante a transformações de locação de $Y$** `r emo::ji("sad")`.]{style="color:gray"}


## Estimador IPW


- $\hat{\tau}^{IPW}$ não é um estimador razoável, pois depende de $c$.
- Uma forma simples de remediar o problema é normalizar os pesos. Este estimador é conhecido como o estimador de Hajek e é dado por $$\hat{\tau}^{Hajek} =  \dfrac{\displaystyle \sum_{i = 1}^n \dfrac{Z_i Y_i}{\hat{e}(X_i)}}{\displaystyle \sum_{i = 1}^n \dfrac{Z_i}{\hat{e}(X_i)}} - \dfrac{\displaystyle \sum_{i = 1}^n \dfrac{(1 - Z_i)Y_i}{1- \hat{e}(X_i)}}{\displaystyle \sum_{i = 1}^n \dfrac{1 - Z_i}{1- \hat{e}(X_i)}}.$$
- Pode-se mostrar que $\hat{\tau}^{Hajek}$ é invariante a transformações de locação `r emo::ji("smile")`. 


## Estimador IPW

- Muitos resultados assintóticos requerem que $0 < \alpha_L <e(X) < \alpha_U < 1$ (condicação conhecida como sobreposição forte).
- Contudo, esta suposição é bastante forte (D'Amour et al. 2021) e mesmo se a condição é verificada para $e(X)$ não há garantia que seja verificada para $\hat{e}(X)$ fazendo com que o estimador _exploda_, tornando os resultados bastante instáveis.
- Para evitar isto, podemos: substituir $\hat{e}(X)$ por $\max [\alpha_L, \min[\hat{e}(X_i), \alpha_U]]$.
- Algumas escolhas comuns para $\alpha_L$ e $\alpha_U$ são 0.1 e 0.9 ou 0.05 e 0.95.


## Estimador IPW

::: {.callout-note}
### Implementação
```{r}
#| echo: true

estimador_ipw <- function(z, y, x, alphas = c(0, 1)) {
  prop_score <- glm(z ~ x, family = binomial)$fitted.values
  prop_score <- pmax(alphas[1], pmin(prop_score, alphas[2]))
  
  tau_ipw <- mean(z * y / prop_score) - mean((1 - z) * y / (1 - prop_score))
  tau_hajek <- mean(z * y / prop_score) / mean(z / prop_score) - mean((1 - z) * y / (1 - prop_score)) / mean((1 - z) / (1 - prop_score))
  
  out <- c(tau_ipw, tau_hajek)
  return(out)
}
```
:::


## Estimador IPW

::: {.callout-note}
### Implementação
```{r}
#| echo: true
bootstrap_ipw <- function(z, y, x, n_boot = 500, alphas = c(0, 1)) {
  poit_estim <- estimador_ipw(z, y, x, alphas)
  
  n <- length(z)
  boot_estim <- replicate(n_boot, {
    sample_boot <- sample(1:n, n, replace = TRUE)
    estimador_ipw(z[sample_boot], y[sample_boot], x[sample_boot, ], alphas)
  })
  boot_se <- apply(boot_estim, 1, sd)
  out <- cbind(poit_estim, boot_se)
  colnames(out) <- c("Est", "SD")
  rownames(out) <- c("IPW", "Hajek")
  return(out)
}
```
:::


## Estimador IPW

::: {.callout-note}
### Exemplo (Merenda e BMI)

[Chang et al. (2016)](https://academic.oup.com/jrsssb/article/78/3/673/7040940?login=false) estudam se a participação no programa da merenda nas escolas (`School_meal`) leva a um aumento no índice de massa muscular (`BMI`) das crianças. 12 outras covariáveis também estão disponíveis no _dataset_.

```{r}
#| echo: true
dados <- read.csv("datasets/nhanes_bmi.csv")[, -1]
z <- dados$School_meal
y <- dados$BMI
x <- scale(dados[, -c(1, 2)])
```

A maneira de exemplo, utilizaremos diversos valores de $\alpha_L$ e $\alpha_U$:
::: {.non-incremenral}
- (0, 1)
- (0.01, 0.99)
- (0.05, 0.95)
- (0.1, 0.9)
:::


:::


## Estimador IPW

::: {.callout-note}
### Exemplo (Merenda e BMI)
```{r}
l_alphas <- list(alpha_01 = c(0, 1), alpha_02 = c(0.01, 0.99),
               alpha_03 = c(0.05, 0.95), alpha_04 = c(0.1, .9))

trim_estim <- lapply(l_alphas, 
                     function(a) {round(bootstrap_ipw(z, y, x, alphas = a), 3)} )
trim_estim
```
:::

- Note que o estimador de Hajek é sempre negativo e insignificante.
- O estimador de HW (IPW) é bastante negativo e, dependendo do truncamento, significativo.
- O estimador de HW é instável e devemos preferir Hajek!


# Propensity Score: Balanceamento
## Propensity Score: Balanceamento


::: {.callout-tip}
### Teorema
O _propensity score_ satisfaz $$Z \perp\!\!\!\perp Z \ e(X).$$ Ademais, para qualquer função $h(\cdot)$, temos que $$\mathbb{E} \Big [\dfrac{Zh(X)}{e(X)} \Big] = \mathbb{E} \Big[\dfrac{(1 - Z) h(X)}{1- e(X)} \Big],$$ desde que ambas as $\mathbb{E}(\cdot)$ existam.
:::

. . . 

<center>
[O que significa essa teorema?]{style="color:red"}
</center>


- Condicional no _propensity score_, a indicadora de tratamento e as covariáveis são independentes.
- Dentro de cada nível do _propensity score_, a distribuição das covariáveis é balanceada entre tratamento e controle.


## Propensity Score: Balanceamento


- O Teorema anterior tem uma implicação direta na análise de dados.
- Antes de termos acesso aos resultados ($Y$), podemos verificar se o modelo de _propensity score_ está bem especificado como para garantir o balanceamento das covariáveis.
- Por exemplo, pense no _propensity score stratification_. Discretizamos $\hat{e}(X)$ para termos $\hat{e}'(X)$ e então, aproximadamente $$Z \perp\!\!\!\perp X | \hat{e}'(X) = e_k \quad (k = 1, \cdots, K).$$ Assim, podemos verificar se a distribuição das covariáveis é a mesma entre tratamento e controle dentro de cada estrato.


## Propensity Score: Balanceamento


```{r}
#| echo: true
library(ggplot2)
balance_check <- sapply(1:ncol(x),
                FUN = function(px){
                  q_prop_score = quantile(prop_score, (1:4)/5)
                  prop_score_strata = cut(prop_score, breaks = c(0, q_prop_score, 1), labels = 1:5)
                  neyman_sre(x[, px], z, prop_score_strata)
                })

dat_balance <- data.frame(est = balance_check[1, ],
                         upper = balance_check[1, ] + 1.96*balance_check[2, ],
                         lower = balance_check[1, ] - 1.96*balance_check[2, ],
                         cov = factor(1:11))
ggplot(dat_balance) + 
  geom_errorbar(aes(x = cov, ymin = lower, ymax = upper), alpha = 0.6) + 
  geom_point(aes(x = cov, y = est), alpha = 0.6) +
  geom_hline(aes(yintercept = 0), alpha = 0.3) + 
  theme_bw() + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.y = element_blank()) +
  xlab("Verificando balanceamento com base na estratificação (K = 5)")                     
```





## Propensity Score: Balanceamento

```{r}
#| echo: true
balance_check <- sapply(1:dim(x)[2],
                FUN = function(px){
                  bootstrap_ipw(z, x[, px], x)[2, ]
                })

dat_balance <- data.frame(est = balance_check[1, ],
                         upper = balance_check[1, ] + 1.96*balance_check[2, ],
                         lower = balance_check[1, ] - 1.96*balance_check[2, ],
                         cov = factor(1:11))
ggplot(dat_balance) + 
  geom_errorbar(aes(x = cov, ymin = lower, ymax = upper), alpha = 0.6) + 
  geom_point(aes(x = cov, y = est), alpha = 0.6) +
  geom_hline(aes(yintercept = 0), alpha = 0.3) + 
  theme_bw() + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.y=element_blank()) +
  xlab("balance check based on weighting")

```


## Referências

::: {.nonincremental}

- Peng Ding (2023). A First Course in Causal Inference. Capítulo 11.




:::