---
title: "Inferência Causal"
subtitle: "Análise de sensibilidade"
author: "Prof. Carlos Trucíos </br> ctrucios@unicamp.br"
Email: "ctrucios@unicamp.br"
institute: "Instituto de Matemática, Estatística e Computação Científica (IMECC), </br> Universidade Estadual de Campinas (UNICAMP)."
toc: true
toc-depth: 1
toc-title: "Conteúdo"
knitr:
    opts_chunk: 
      fig.align: 'center'
execute:
    message: false
    warning: false
format:
    revealjs:
        slide-number: true
        show-slide-number: print
        self-contained: false
        chalkboard: true
        width: 1600
        height: 900
        theme: [default, styles.scss]
        incremental: true
        code-fold: true
        logo: "imagens/imecc.png"
        footer: "Carlos Trucíos (IMECC/UNICAMP)  |  ME920/MI628 - Inferência Causal |  [ctruciosm.github.io](https://ctruciosm.github.io/)"
        highlight-style: "a11y"
        title-slide-attributes:
            data-background-image: "imagens/unicamp.png"
            data-background-size: 20%
            data-background-position: 99% 5%
            data-background-opacity: "1"
---



# Introdução
## Introdução

Sejam $\{Z_i, X_i, Y_i(1), Y_i(0) \}_{i = 1}^n \sim IID \{Z, X, Y(1), Y(0) \}$, $n$ observações de um estudo observacional e seja $\tau$ o efeito causal médio, $$\tau = \mathbb{E} \big [ Y(1) - Y(0) \big ].$$

. . . 

Equivalentemente, podemos re-escrever $\tau$ como 
\begin{align}
\tau  &= \underbrace{\mathbb{E} [Y | Z = 1]P(Z = 1) + \mathbb{E}[Y(1) | Z = 0] P(Z = 0)}_{\mathbb{E}[Y(1)]} \\
      &- \underbrace{\Big [ \mathbb{E}[Y(0) | Z = 1]P(Z = 1) + \mathbb{E} [Y | Z = 0] P(Z = 0) \Big ]}_{\mathbb{E}[Y(0)]},
\end{align}

em que a dificuldade é estimar os contrafactuais $\mathbb{E}[Y(1) | Z = 0]$ e $\mathbb{E}[Y(0) | Z = 1].$
 

## Introdução

Existem duas estratégias para estimar $\mathbb{E}[Y(1) | Z = 0]$ e $\mathbb{E}[Y(0) | Z = 1].$

. . . 

1. **Primeira Estratégia:** Assumir ignorabilidade
2. **Segunda Estratégia:** não assume nada além de que os resultados são limitados entre $\underline{y}$ e $\overline{y}$ (foco da aula de hoje).

. . . 

Se $Y$ for binário, naturalmente $\underline{y} = 0$ e $\overline{y} = 1$.

. . . 


Sob esta suposição, $\mathbb{E}[Y(1) | Z = 0]$ e $\mathbb{E}[Y(0) | Z = 1]$ são também limitados entre $\underline{y} = 0$ e $\bar{y} = 1$, sendo estes limites os casos mais extremos que $\underline{y} = 0$ e $\overline{y} = 1$ podem assumir.


# Manski
## Manski

Assuma que os resultados são limitados entre $\underline{y} = 0$ e $\overline{y} = 1$.

. . . 

Então,


$$\mathbb{E}[Y(1)] = \mathbb{E}[Y | Z = 1]P(Z = 1) + \underbrace{\mathbb{E}[Y(1) | Z = 0]}_{\geq \underline{y}}P(Z = 0), \quad e$$

. . . 


$$\mathbb{E}[Y(1)] = \mathbb{E}[Y | Z = 1]P(Z = 1) + \underbrace{\mathbb{E}[Y(1) | Z = 0]}_{\leq \overline{y}}P(Z = 0).$$

. . . 


Assim, 

$$\mathbb{E}[Y | Z = 1]P(Z = 1) + \underline{y}P(Z = 0) \leq \mathbb{E}[Y(1)] \leq \mathbb{E}[Y | Z = 1]P(Z = 1) + \overline{y}P(Z = 0)$$

## Manski

De forma semelhante, 

$$\mathbb{E}[Y | Z = 0]P(Z = 0) + \underline{y}P(Z = 1) \leq \mathbb{E}[Y(0)] \leq \mathbb{E}[Y | Z = 0]P(Z = 0) + \overline{y}P(Z = 1)$$

. . . 

Assim, 


\begin{align}
\mathbb{E}[Y | Z = 1]P(Z = 1) & + \underline{y}P(Z = 0) - \mathbb{E}[Y | Z = 0]P(Z = 0) - \overline{y}P(Z = 1) \\
& \leq \underbrace{\mathbb{E}[Y(1) - Y(0)]}_{\tau} \leq \\
\mathbb{E}[Y | Z = 1]P(Z = 1) & + \overline{y}P(Z = 0) - \mathbb{E}[Y | Z = 0]P(Z = 0) - \underline{y}P(Z = 1)
\end{align}


. . . 

Sem suposições adicionais, a distribuição dos dados não determina unicamente $\tau$. Neste caso, dizemos que $\tau$ é parcialmente identificável.


## Manski


::: {.callout-tip}
### Definição: identificação

Um parâmetro $\theta$ é dito identificável se pode ser escrito como uma função da distribuição dos dados observados sob certas suposições do modelo. Um parâmetro $\theta$ é dito identificável não parametricamente se pode ser escrito como uma função da distribuição dos dados observados sem quaisquer suposição do modelo.
:::

::: {.callout-tip}
### Definição: identificação parcial.

Um parâmetro $\theta$ é dito parcialmente identificável se a distribuição dos dados observados é compativel com múltiplos valores de $\theta$.
:::


. . . 


<center>
[Se o parâmetro $\theta$ for unicamente determinado pela distribuição dos dados observados é **identificável**. Caso contrário, é apenas **parcialmente identificável**. Assim, $\tau$ é idenficiável sob a suposição de ignorabilidade, mas apenas parcialmente identificável sem a suposição de ignorabilidade]{style="color:red;"}
</center>


## Manski

A ideia de limitar parâmetros causais com suposições mínimas foi explorada por Manski ([1990](https://www.jstor.org/stable/2006592), [2003](https://books.google.com.br/books?hl=pt-BR&lr=&id=Zr0H7jMBqjkC&oi=fnd&pg=PA1&dq=manski+2003+partial+identification&ots=wONbugMojH&sig=dPV81Et6l_Wm-9NLi74EtkQn5tg&redir_esc=y#v=onepage&q=manski%202003%20partial%20identification&f=false)) e é uma ferramente poderosa quando acompanhada de outras suposições qualitativas.

. . . 


::: {.callout-note}
### Exemplo

Podemos acreditar que o tratamento não faz mal para nenhuma unidade experimental, ou seja $$Y(1) \leq Y(0),$$

Desta forma, o limite inferior de $\tau$ é zero mas o limite superior ainda é dado por $$\mathbb{E}[Y | Z = 1]P(Z = 1) + \overline{y}P(Z = 0) - \mathbb{E}[Y | Z = 0]P(Z = 0) - \underline{y}P(Z = 1)$$

:::

# Análise de sensibilidade
## Análise de sensibilidade

Sejam os parâmetros de sensibilidade, 


$$\varepsilon_1(X) = \dfrac{\mathbb{E} \big[ Y(1) | Z = 1, X \big ]}{\mathbb{E} \big[ Y(1) | Z = 0, X \big ]} \quad e \quad \varepsilon_0(X) = \dfrac{\mathbb{E} \big[ Y(0) | Z = 1, X \big ]}{\mathbb{E} \big[ Y(0) | Z = 0, X \big ]}.$$


. . . 

::: {.callout-tip}
### Teorema
Com $\varepsilon_1(X)$ e $\varepsilon_0(X)$ conhecidos, temos que

<center>
\begin{align}


\end{align}
</center>

:::



## Referências

::: {.nonincremental}

- Peng Ding (2023). A First Course in Causal Inference. Capítulo 18.

:::